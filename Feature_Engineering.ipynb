{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Engineering\n",
        "\n",
        "**1. What is a parameter?**\n",
        "   - In machine learning, a parameter refers to a variable that the model learns from the training data.\n",
        "\n",
        "For example, in a linear regression model:\n",
        "\n",
        "y = mx + b\n",
        "\n",
        "- m (slope) and b (intercept) are parameters.\n",
        "- These values are learned during training by minimizing the error between predictions and actual values.\n",
        "\n",
        "Key points:\n",
        "- Parameters are internal to the model.\n",
        "- They are automatically optimized during training.\n",
        "- Examples:\n",
        "  - Weights in a neural network\n",
        "  - Coefficients in linear or logistic regression\n",
        "---\n",
        "**2. What is correlation? What does negative correlation mean?**\n",
        "  - Correlation is a statistical measure that shows how two variables are related to each other — whether they increase or decrease together, or are not related at all.\n",
        "- It’s usually measured using Pearson’s correlation coefficient (r), which ranges from -1 to +1:\n",
        "       - +1 means perfect positive correlation  \n",
        "       - 0 means no correlation  \n",
        "       - -1 means perfect negative correlation\n",
        "\n",
        "- Negative correlation means that as one variable increases, the other decreases, and vice versa.\n",
        "\n",
        "For example:\n",
        "- As the number of hours watching TV increases, exam scores might decrease.\n",
        "- As temperature drops, sales of jackets may increase.\n",
        "---\n",
        "**3. Define Machine Learning. What are the main components in Machine Learning?**\n",
        " - Machine Learning is a branch of artificial intelligence that allows systems to learn from data and make decisions or predictions without being explicitly programmed.\n",
        "\n",
        "- In other words, instead of writing fixed rules, the system learns patterns from data to perform tasks.\n",
        "\n",
        "\n",
        "\n",
        " - Main Components in Machine Learning:\n",
        "- Data  \n",
        "    - The foundation of any machine learning system. It can be labeled (for supervised learning) or unlabeled (for unsupervised learning).\n",
        "- Model  \n",
        "    - A mathematical representation that learns from the data and makes predictions or decisions.\n",
        "- Features  \n",
        "    - The input variables or characteristics used to train the model.\n",
        "- Algorithm  \n",
        "    - A method or procedure, like linear regression or decision trees, that the model uses to learn from data.\n",
        "- Training  \n",
        "    - The process where the model learns patterns from the training dataset.\n",
        "- Evaluation  \n",
        "    - Testing the model on unseen data to check how well it performs.\n",
        "- Prediction  \n",
        "    - Once trained, the model is used to make predictions on new data.\n",
        "\n",
        "---\n",
        "**4.  How does loss value help in determining whether the model is good or not?**\n",
        "- The loss value helps in understanding how well or poorly a machine learning model is performing during training or testing.\n",
        "\n",
        "\n",
        " - Workings :\n",
        "- The loss is a number that represents the difference between the model’s prediction and the actual value. It is calculated using a loss function, such as Mean Squared Error or Cross-Entropy.\n",
        "- A low loss value means the model's predictions are close to the actual results, so the model is doing well.  \n",
        "- A high loss value means the predictions are far from the actual results, indicating poor performance.\n",
        "\n",
        "\n",
        " - It helps determine model quality:\n",
        "- Monitors training progress  \n",
        "   If the loss keeps decreasing during training, it shows that the model is learning correctly.\n",
        "- Helps in tuning  \n",
        "   A consistently high loss suggests that the model may need better features, more data, or different hyperparameters.\n",
        "- Guides optimization  \n",
        "   Loss is what the training algorithm, like gradient descent, tries to minimize. So, lower loss means a better model.\n",
        "- Comparison tool  \n",
        "   You can compare loss values across different models or configurations to choose the best one.\n",
        "\n",
        "---\n",
        "**5. What are continuous and categorical variables?**\n",
        "  - Continuous variables are numerical values that can take any value within a range. They are often measured and can have decimals.\n",
        "\n",
        "Examples:  \n",
        "- Height  \n",
        "- Weight  \n",
        "- Temperature  \n",
        "- Age (if measured precisely)\n",
        "\n",
        "These values can be split infinitely (like 22.5, 22.55, 22.555, etc.).\n",
        "\n",
        "\n",
        " - Categorical variables represent categories or groups. They are not measured but labeled or classified. These can be:\n",
        "- Nominal: No specific order (like colors: red, green, blue)  \n",
        "- Ordinal: Have a meaningful order (like low, medium, high)\n",
        "\n",
        "Examples:  \n",
        "- Gender (male, female)  \n",
        "- City (Mumbai, Delhi, Chennai)  \n",
        "- Education level (High School, Graduate, Postgraduate)\n",
        "\n",
        "---\n",
        "**6. How do we handle categorical variables in Machine Learning? What are the common techniques?**\n",
        " - Handling categorical variables in machine learning is important because most algorithms require numerical inputs. There are several techniques for converting categorical data into a format that models can understand.\n",
        "\n",
        "\n",
        " - Common techniques to handle categorical variables:\n",
        "- Label Encoding  \n",
        "    - This technique assigns a unique integer value to each category. It’s simple but can introduce an implicit ordinal relationship where none exists.\n",
        "\n",
        "   Example:  \n",
        "   Colors: Red = 0, Green = 1, Blue = 2  \n",
        "   This method is suitable for ordinal variables, where there is a meaningful order.\n",
        "- One-Hot Encoding  \n",
        "    - One-hot encoding creates new binary columns for each category. If the category exists, it’s marked with a 1; otherwise, it’s 0.\n",
        "\n",
        "   Example:  \n",
        "   Colors:  \n",
        "   Red → [1, 0, 0]  \n",
        "   Green → [0, 1, 0]  \n",
        "   Blue → [0, 0, 1]  \n",
        "   This method works well for nominal variables, where there's no natural order.\n",
        "- Binary Encoding  \n",
        "    - This method first converts categories into numbers, then encodes those numbers in binary format. It’s more memory-efficient than one-hot encoding for variables with many categories.\n",
        "\n",
        "   Example:  \n",
        "   If categories are Red, Green, and Blue, you convert them to 1, 2, 3 and then into binary:  \n",
        "   Red → 01  \n",
        "   Green → 10  \n",
        "   Blue → 11\n",
        "- Count (Frequency) Encoding  \n",
        "    - In this technique, each category is replaced with the frequency or count of its occurrences in the dataset. This is helpful when some categories are more frequent than others.\n",
        "\n",
        "   Example:  \n",
        "   Colors:  \n",
        "   Red → 5  \n",
        "   Green → 10  \n",
        "   Blue → 15\n",
        "- Target Encoding (Mean Encoding)  \n",
        "   - This method replaces categories with the mean of the target variable for that category. It can lead to better model performance but should be used carefully to avoid data leakage.\n",
        "\n",
        "   Example:  \n",
        "   If the target variable is sale amount, then the encoding for a color might be the average sales of that color.\n",
        "\n",
        "---\n",
        "**7. What do you mean by training and testing a dataset?**\n",
        "   -  Training and testing a dataset are two important steps in building and evaluating a machine learning model.\n",
        "\n",
        "\n",
        " - Training a dataset means using a portion of the data to teach the model. The model learns patterns, relationships, and structures from this data to make predictions.\n",
        "- Example: If you're training a model to predict house prices, the training data would include house features like area, location, number of rooms, and their actual prices.\n",
        "\n",
        "\n",
        " - Testing a dataset means using a separate portion of the data (not seen during training) to evaluate how well the model performs on new, unseen data. This helps check if the model has really learned or just memorized the training data.\n",
        "- Continuing the house price example: The testing data would have similar house features, and the model will try to predict the price. Then, we compare the prediction to the actual price to measure accuracy.\n",
        "\n",
        "---\n",
        "\n",
        "**8. What is sklearn.preprocessing?**\n",
        "  - \"sklearn.preprocessing\" is a module in the Scikit-learn library that provides tools for scaling, transforming, and encoding data before feeding it into a machine learning model.\n",
        "- Since most models work better with properly formatted and scaled data, this module helps prepare your dataset in a way that improves model performance.\n",
        "\n",
        "\n",
        " - Some commonly used functions in \"sklearn.preprocessing\" :\n",
        "- StandardScaler  \n",
        "    - Scales features to have mean 0 and standard deviation 1.\n",
        "- MinMaxScaler  \n",
        "    - Scales features to a fixed range, usually 0 to 1.\n",
        "- LabelEncoder  \n",
        "    - Converts categorical labels (like \"yes\", \"no\") into numeric values (like 1, 0).\n",
        "- OneHotEncoder  \n",
        "    - Converts categorical features into a set of binary columns.\n",
        "- OrdinalEncoder  \n",
        "    - Assigns ordered numerical values to categories, useful for ordinal data.\n",
        "- Binarizer  \n",
        "    - Converts numerical values into binary (0 or 1) based on a threshold.\n",
        "- PolynomialFeatures  \n",
        "    - Generates polynomial combinations of features, useful for feature engineering.\n",
        "\n",
        "---\n",
        "\n",
        "**9. What is a Test set?**\n",
        "  - A test set is a portion of your dataset that is used to evaluate the performance of your machine learning model after it has been trained.\n",
        "\n",
        "- It is not used during training, so it acts like new, unseen data, helping you understand how well the model might perform in the real world.\n",
        "\n",
        "We use a test set :\n",
        "\n",
        "- To check if the model has truly learned patterns or just memorized the training data  \n",
        "- To estimate how the model will perform on actual, future data  \n",
        "- To compare different models or tuning settings fairly\n",
        "\n",
        "Example:\n",
        "\n",
        "Imagine you have 1,000 rows of data. You might split it like this:\n",
        "\n",
        "- 800 rows for training  \n",
        "- 200 rows for testing  \n",
        "\n",
        "The model is trained on the 800 rows and then tested on the remaining 200 to check its accuracy, precision, recall, and so on.\n",
        "\n",
        "---\n",
        "**10.  How do we split data for model fitting (training and testing) in Python?  How do you approach a Machine Learning problem?**\n",
        "  - In Python, we commonly use Scikit-learn’s `train_test_split` function to split the dataset.\n",
        "\n",
        "Example:\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X is the feature set, y is the target variable\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "```\n",
        "\n",
        "test_size=0.2 means 20 percent of the data is reserved for testing, and 80 percent is used for training.  \n",
        "\n",
        "random_state=42 ensures reproducibility so you get the same split every time you run the code.\n",
        "\n",
        "\n",
        " - Here’s a typical step-by-step approach:\n",
        "- Understand the problem  \n",
        "    - Clearly define the problem you are solving, like classification or regression.\n",
        "- Collect and explore data  \n",
        "    - Gather the dataset, explore it using summaries and visualizations, and check for missing values.\n",
        "- Preprocess the data  \n",
        "    - Handle missing values, encode categorical variables, scale features, and split the data.\n",
        "- Choose a model  \n",
        "    - Select a model depending on your problem, such as linear regression or decision tree.\n",
        "- Train the model  \n",
        "    - Fit the model using the training data.\n",
        "- Evaluate the model  \n",
        "    - Test it using the test set and calculate evaluation metrics such as accuracy, precision, recall, or RMSE.\n",
        "- Tune the model  \n",
        "    - Improve performance using cross-validation, grid search, or hyperparameter tuning.\n",
        "- Deploy the model  \n",
        "    - Once satisfied with performance, deploy it to make real-world predictions.\n",
        "- Monitor and maintain  \n",
        "    - Keep track of the model’s performance over time and retrain when necessary.\n",
        "\n",
        "---\n",
        "**11. Why do we have to perform EDA before fitting a model to the data?**\n",
        "  - EDA, or Exploratory Data Analysis, is a very important step before fitting a machine learning model because it helps you understand the data better and make informed decisions during preprocessing and modeling.\n",
        "\n",
        "\n",
        " - EDA is necessary before model fitting:\n",
        "- Understand the structure of the data  \n",
        "    - EDA gives you an overview of the dataset: number of rows, columns, data types, and feature distributions.\n",
        "- Detect missing or inconsistent data  \n",
        "    - You can find missing values, duplicates, or incorrect data entries that could affect model performance.\n",
        "- Identify relationships between variables  \n",
        "    - EDA helps reveal how features relate to each other and to the target variable. This helps in feature selection and engineering.\n",
        "- Spot outliers or unusual patterns  \n",
        "    - Outliers can negatively impact model accuracy. EDA helps detect and handle them appropriately.\n",
        "- Choose the right encoding and scaling techniques  \n",
        "    - By understanding variable types (categorical or continuous), you can decide how to encode or scale them.\n",
        "- Avoid wrong assumptions  \n",
        "    - Sometimes assumptions like \"more data means better model\" don’t hold if the data quality is bad. EDA helps verify quality.\n",
        "- Decide modeling strategy  \n",
        "    - Based on your insights from EDA, you can choose which models or techniques might work best.\n",
        "\n",
        "---\n",
        "\n",
        "**12. What is correlation?**\n",
        "  - Correlation is a statistical measure that describes the strength and direction of a relationship between two variables.\n",
        "\n",
        "- In machine learning and data analysis, correlation helps us understand how two features move in relation to each other. It’s especially useful during feature selection to identify redundant or strongly related variables.\n",
        "\n",
        "\n",
        " - Correlation tells us:\n",
        "- If two variables increase or decrease together, they have a positive correlation.  \n",
        "- If one variable increases while the other decreases, they have a negative correlation.  \n",
        "- If the variables have no predictable pattern, the correlation is close to zero.\n",
        "\n",
        "\n",
        "  - Correlation values range between -1 and +1:\n",
        "- +1 means a perfect positive relationship  \n",
        "- -1 means a perfect negative relationship  \n",
        "- 0 means no relationship at all\n",
        "\n",
        "\n",
        " - Example:  \n",
        "- If study time and exam score have a correlation of 0.85, it means students who study more tend to score higher.  \n",
        "- If age and interest in video games have a correlation of -0.60, it suggests older people are less likely to play video games.\n",
        "\n",
        "---\n",
        "\n",
        "**13. What does negative correlation mean?**\n",
        "  - Negative correlation means that as one variable increases, the other decreases. In other words, the two variables move in opposite directions.\n",
        "\n",
        "\n",
        "- Example:  \n",
        " - If the number of hours spent watching TV goes up, and grades in school go down, these two might have a negative correlation.  \n",
        " -If the temperature decreases and the sales of hot coffee increase, that's also a negative correlation.\n",
        "\n",
        "\n",
        " - Numeric value:  \n",
        "- A negative correlation is represented by a correlation coefficient between 0 and -1.  \n",
        "- A value close to -1 means a strong negative correlation.  \n",
        "- A value close to 0 means the relationship is weak or there is no correlation.\n",
        "\n",
        " - So, when you see a negative correlation in your data, it means that higher values of one feature are generally associated with lower values of another.\n",
        "\n",
        "---\n",
        "\n",
        "**14. How can you find correlation between variables in Python?**\n",
        "  - Basic Example using `pandas`:\n",
        "\n",
        "```python\n",
        "import pandas as pd\n",
        "\n",
        "# Sample DataFrame\n",
        "data = {\n",
        "    'Hours_Studied': [2, 4, 6, 8, 10],\n",
        "    'Exam_Score': [50, 60, 70, 80, 90],\n",
        "    'TV_Watched': [8, 6, 4, 2, 1]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Calculate correlation\n",
        "correlation_matrix = df.corr()\n",
        "\n",
        "print(correlation_matrix)\n",
        "```\n",
        "\n",
        "- Output will look like this:\n",
        "\n",
        "```\n",
        "               Hours_Studied  Exam_Score  TV_Watched\n",
        "Hours_Studied         1.000      1.000     -0.984\n",
        "Exam_Score            1.000      1.000     -0.984\n",
        "TV_Watched           -0.984     -0.984      1.000\n",
        "```\n",
        "\n",
        "This shows:\n",
        "- A strong \"positive correlation\" between `Hours_Studied` and `Exam_Score`\n",
        "- A strong \"negative correlation\" between `TV_Watched` and both `Hours_Studied` and `Exam_Score`\n",
        "\n",
        "---\n",
        "**15. What is causation? Explain difference between correlation and causation with an example ?**\n",
        "- Causation means that one variable directly affects or causes a change in another.\n",
        "\n",
        "- In other words, if variable A causes variable B to change, then there's causation between A and B.\n",
        "\n",
        "- Difference Between Correlation and Causation:\n",
        "\n",
        "| Feature       | Correlation                              | Causation                                  |\n",
        "|---------------|------------------------------------------|---------------------------------------------|\n",
        "| Meaning       | Two variables move together              | One variable causes the change in another   |\n",
        "| Direction     | Can be positive, negative, or zero       | Always has a cause-effect direction         |\n",
        "| Implication   | Does not imply cause and effect          | Implies direct influence or effect          |\n",
        "\n",
        "- Example:\n",
        "\n",
        "- Correlation (No Causation):  \n",
        "  Ice cream sales and drowning cases are positively correlated.  \n",
        "  But eating ice cream doesn’t cause drowning.  \n",
        "  The real cause is hot weather, which increases both swimming and ice cream sales.\n",
        "\n",
        "- Causation:  \n",
        "  Smoking causes lung damage.  \n",
        "  This is backed by scientific evidence, so smoking and lung disease have causation.\n",
        "\n",
        "---\n",
        "**16. What is an Optimizer? What are different types of optimizers? Explain each with an example ?**\n",
        "  - An optimizer is a technique used in machine learning to adjust the model’s parameters (like weights and biases) in order to minimize the loss function. The goal is to make the model's predictions as accurate as possible.\n",
        "- In simple terms, the optimizer helps the model learn by improving how well it performs with each step.\n",
        "\n",
        "\n",
        "- Types of Optimizers:\n",
        " - Gradient Descent  \n",
        "It is the most basic and widely used optimizer. It calculates the gradient (slope) of the loss function and updates the parameters in the direction that reduces the loss.\n",
        "\n",
        "Formula:  \n",
        "new_weight = old_weight - learning_rate * gradient\n",
        "\n",
        "Example:\n",
        "```python\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "```\n",
        "\n",
        "2. Stochastic Gradient Descent (SGD)  \n",
        "Unlike batch gradient descent (which uses all data), SGD updates weights using one data point at a time. It’s faster but more noisy.\n",
        "\n",
        "Example:\n",
        "```python\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
        "```\n",
        "\n",
        "3. Mini-Batch Gradient Descent  \n",
        "A combination of both batch and SGD — it updates weights using small chunks of data. This balances speed and stability.\n",
        "\n",
        "Used internally in most training loops.\n",
        "\n",
        "4. Momentum  \n",
        "Momentum adds a \"memory\" of past gradients to speed up training and avoid oscillations.\n",
        "\n",
        "Example:\n",
        "```python\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.9)\n",
        "```\n",
        "\n",
        "5. RMSprop (Root Mean Square Propagation)  \n",
        "It adapts the learning rate for each parameter by dividing the gradient by a moving average of past squared gradients. Works well for RNNs.\n",
        "\n",
        "Example:\n",
        "```python\n",
        "optimizer = tf.keras.optimizers.RMSprop(learning_rate=0.001)\n",
        "```\n",
        "\n",
        "6. Adam (Adaptive Moment Estimation)  \n",
        "One of the most popular optimizers. It combines Momentum and RMSprop. It adjusts learning rates for each parameter and maintains running averages of both gradients and their squares.\n",
        "\n",
        "Example:\n",
        "```python\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**17. What is sklearn.linear_model?**\n",
        "   - `sklearn.linear_model` is a module in the scikit-learn library that provides classes and functions to implement linear models for regression and classification tasks.\n",
        "\n",
        "- It includes machine learning models that try to draw a straight line (or a hyperplane) through the data to make predictions.\n",
        "\n",
        "- Common Models in `sklearn.linear_model`:\n",
        "- LinearRegression  :\n",
        "Used for predicting continuous values using a straight-line relationship.  \n",
        "Example: Predicting house prices based on size.\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "```\n",
        "\n",
        "- LogisticRegression  :\n",
        "Used for binary or multiclass classification problems.  \n",
        "Example: Predicting if an email is spam or not.\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)\n",
        "```\n",
        "\n",
        "- Ridge Regression (Ridge)  :\n",
        "Linear regression with L2 regularization (helps reduce overfitting).\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "model = Ridge(alpha=1.0)\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "- Lasso Regression (Lasso) :\n",
        "Linear regression with L1 regularization (can shrink some coefficients to 0, useful for feature selection).\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "model = Lasso(alpha=0.1)\n",
        "model.fit(X_train, y_train)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**18.  What does model.fit() do? What arguments must be given?**\n",
        "   - The \".fit()\" function in machine learning is used to train a model on the provided data. It allows the model to learn the relationship between the input features (X) and the target/output (y).\n",
        "\n",
        "- Syntax :\n",
        "\n",
        "```python\n",
        "model.fit(X, y)\n",
        "```\n",
        "\n",
        "- Arguments:\n",
        "\n",
        "- X – Input features (independent variables)  \n",
        "   - Type: array-like (2D), shape: (n_samples, n_features)  \n",
        "   - Example: data like age, income, number of hours studied, etc.\n",
        "\n",
        "- y – Target values (dependent variable)  \n",
        "   - Type: array-like (1D or 2D depending on the task)  \n",
        "   - Example: labels like price, category, or pass/fail\n",
        "\n",
        "- Example:\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = [[1], [2], [3], [4], [5]]\n",
        "y = [2, 4, 6, 8, 10]\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X, y)\n",
        "```\n",
        "\n",
        "In this example, the model learns the pattern between the input X and the output y.\n",
        "\n",
        "---\n",
        "\n",
        "**19. What does model.predict() do? What arguments must be given?**\n",
        "   - The `model.predict()` function is used after training the model (using `.fit()`) to make predictions on new or unseen data.\n",
        "\n",
        "- It takes input features (X) and returns the predicted output (ŷ) based on the patterns the model has learned.\n",
        "\n",
        "- Syntax:\n",
        "\n",
        "```python\n",
        "predictions = model.predict(X_new)\n",
        "```\n",
        "\n",
        "- Arguments:\n",
        "\n",
        "- X_new – The input data for which you want to make predictions.  \n",
        "  - Type: array-like or DataFrame  \n",
        "  - Shape: (n_samples, n_features)  \n",
        "  - It must have the same number of features as the data used during training.\n",
        "\n",
        "- Example:\n",
        "\n",
        "```python\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X_train = [[1], [2], [3]]\n",
        "y_train = [2, 4, 6]\n",
        "\n",
        "X_test = [[4], [5]]\n",
        "\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "predictions = model.predict(X_test)\n",
        "print(predictions)  # Output: [8. 10.]\n",
        "```\n",
        "\n",
        "\n",
        " - Output:\n",
        "- Returns an array of predicted values\n",
        "- Output shape depends on the model and type of task (regression or classification)\n",
        "\n",
        "---\n",
        "**20. What are continuous and categorical variables?**\n",
        "  - In machine learning and statistics, variables (also called features) are typically classified into two main types: continuous and categorical.\n",
        "\n",
        "\n",
        " - Continuous Variables:\n",
        "- These are numeric variables that can take any value within a range.\n",
        "- The values are measurable and infinite (in theory).\n",
        "- Examples:\n",
        "  - Height (e.g., 165.4 cm)\n",
        "  - Temperature (e.g., 36.6°C)\n",
        "  - Age, weight, income, time, etc.\n",
        "\n",
        "Example:\n",
        "```python\n",
        "age = [23, 34, 45, 28]\n",
        "price = [12000.5, 13499.9, 11000.0, 15000.0]\n",
        "```\n",
        "\n",
        "\n",
        " - Categorical Variables:\n",
        "- These are variables that represent groups or categories.\n",
        "- The values are discrete and limited.\n",
        "- Can be:\n",
        "  - Nominal: No order (e.g., gender, city, color)\n",
        "  - Ordinal: Ordered categories (e.g., low, medium, high)\n",
        "\n",
        "Examples:\n",
        "  - Gender (Male, Female)\n",
        "  - Product category (Electronics, Furniture)\n",
        "  - Education level (High School, Graduate, Postgraduate)\n",
        "\n",
        "Example:\n",
        "```python\n",
        "gender = ['Male', 'Female', 'Female', 'Male']\n",
        "city = ['Delhi', 'Mumbai', 'Chennai', 'Delhi']\n",
        "```\n",
        "\n",
        "---\n",
        "**21. What is feature scaling? How does it help in Machine Learning?**\n",
        "  - Feature scaling is a preprocessing technique in machine learning used to normalize or standardize the range of independent variables (features).\n",
        "\n",
        "- Different features in a dataset may have different units or ranges — and this can negatively affect the performance of many machine learning models.\n",
        "\n",
        "\n",
        " - Feature Scaling is Important:\n",
        "- Some models (like KNN, SVM, Logistic Regression, and Gradient Descent-based models) are sensitive to the scale of the data.\n",
        "- Features with larger values can dominate the learning process and bias the model.\n",
        "- It helps in faster convergence and better accuracy during training.\n",
        "\n",
        "\n",
        " - Common Techniques for Feature Scaling:\n",
        "- Min-Max Scaling (Normalization) :  \n",
        "   Scales all values between 0 and 1.  \n",
        "   Formula:  \n",
        "   \\[\n",
        "   X_{\\text{scaled}} = \\frac{X - X_{\\min}}{X_{\\max} - X_{\\min}}\n",
        "   \\]\n",
        "   Example:\n",
        "   ```python\n",
        "   from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "   scaler = MinMaxScaler()\n",
        "   X_scaled = scaler.fit_transform(X)\n",
        "   ```\n",
        "- Standardization (Z-score Normalization) :\n",
        "   Centers data around the mean with unit variance.  \n",
        "   Formula:  \n",
        "   \\[\n",
        "   X_{\\text{scaled}} = \\frac{X - \\mu}{\\sigma}\n",
        "   \\]\n",
        "   Example:\n",
        "   ```python\n",
        "   from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "   scaler = StandardScaler()\n",
        "   X_scaled = scaler.fit_transform(X)\n",
        "   ```\n",
        "\n",
        " - When Not to Use Feature Scaling:\n",
        "- Tree-based models like Decision Trees, Random Forest, and XGBoost do not require feature scaling because they split data based on thresholds, not distances.\n",
        "\n",
        "---\n",
        "**22. How do we perform scaling in Python?**\n",
        "   - Scaling can be performed using the `sklearn.preprocessing` module, which provides the necessary classes and functions to apply various scaling techniques like Min-Max Scaling and Standardization.\n",
        "\n",
        "- Min-Max Scaling (Normalization) :\n",
        "\n",
        "This scales the data between a specific range, usually [0, 1].\n",
        "\n",
        "- Example:\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Example dataset\n",
        "X = [[1], [2], [3], [4], [5]]\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(X_scaled)\n",
        "```\n",
        "\n",
        "- Standardization (Z-score Normalization) :\n",
        "\n",
        "This scales the data so that it has a mean of 0 and a standard deviation of 1.\n",
        "\n",
        "- Example:\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Example dataset\n",
        "X = [[1], [2], [3], [4], [5]]\n",
        "\n",
        "# Initialize the StandardScaler\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(X_scaled)\n",
        "```\n",
        "\n",
        "- Handling Multiple Features (Multiple Columns) :\n",
        "\n",
        "Scaling can be applied to datasets with multiple features (columns).\n",
        "\n",
        "\n",
        " - Example for Multiple Features:\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Example dataset with multiple features\n",
        "X = [[1, 200], [2, 300], [3, 400], [4, 500], [5, 600]]\n",
        "\n",
        "# Initialize the MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "print(X_scaled)\n",
        "```\n",
        "\n",
        "---\n",
        "**23. What is sklearn.preprocessing?**\n",
        "  - `sklearn.preprocessing` is a module in the scikit-learn library that provides a collection of functions and classes used for preprocessing data before training machine learning models.\n",
        "\n",
        "- It helps in scaling, transforming, encoding, and normalizing data to make it suitable for machine learning algorithms.\n",
        "\n",
        "- Common Tasks You Can Do with `sklearn.preprocessing`:\n",
        "\n",
        "- Scaling/Normalizing Features :\n",
        "   - `StandardScaler`: Standardizes features (mean = 0, standard deviation = 1)\n",
        "   - `MinMaxScaler`: Scales features to a fixed range (usually 0 to 1)\n",
        "   - `RobustScaler`: Scales features using median and IQR, useful when data has outliers\n",
        "\n",
        "- Encoding Categorical Features :\n",
        "   - `LabelEncoder`: Converts class labels (target) to numbers\n",
        "   - `OneHotEncoder`: Converts categorical variables to one-hot encoded format (dummy variables)\n",
        "   - `OrdinalEncoder`: Encodes categorical features with an ordinal relationship\n",
        "\n",
        "- Handling Missing Values :\n",
        "   - `SimpleImputer`: Fills missing values with strategies like mean, median, or a constant\n",
        "\n",
        "- Polynomial and Custom Feature Creation :\n",
        "   - `PolynomialFeatures`: Creates polynomial and interaction features from existing features\n",
        "\n",
        "- Example: Scaling and Encoding :\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "# Scaling\n",
        "data = np.array([[1.0, 20.0], [2.0, 30.0], [3.0, 40.0]])\n",
        "scaler = StandardScaler()\n",
        "scaled_data = scaler.fit_transform(data)\n",
        "\n",
        "# One-hot encoding\n",
        "encoder = OneHotEncoder()\n",
        "encoded = encoder.fit_transform([['Male'], ['Female'], ['Female']]).toarray()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "**24. How do we split data for model fitting (training and testing) in Python?**\n",
        "  - In Python, you can split your dataset into training and testing sets using the `train_test_split` function from `sklearn.model_selection`. This step is essential in machine learning to evaluate how well your model performs on unseen data.\n",
        "\n",
        "\n",
        " - Split the Data :\n",
        "- Training set: Used to train the machine learning model.\n",
        "- Testing set: Used to evaluate the model’s performance on unseen data.\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Suppose X is your features and y is your target variable\n",
        "# Example:\n",
        "# X = [[1], [2], [3], [4], [5]]\n",
        "# y = [10, 20, 30, 40, 50]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "```\n",
        "\n",
        "\n",
        " - Parameters Explained:\n",
        "- `X`: Feature data (independent variables)\n",
        "- `y`: Target variable (dependent variable)\n",
        "- `test_size`: Proportion of the dataset to include in the test split (e.g., 0.2 = 20% test, 80% train)\n",
        "- `random_state`: Controls the shuffling process (helps with reproducibility)\n",
        "\n",
        "\n",
        " - Optional Parameter:\n",
        "- `shuffle=True`: Shuffles the data before splitting (default is True)\n",
        "\n",
        "- Example Output:\n",
        "\n",
        "If you had 100 rows and used `test_size=0.2`, the function would split:\n",
        "- 80 rows → Training data\n",
        "- 20 rows → Testing data\n",
        "\n",
        "---\n",
        "\n",
        "**25. Explain data encoding?**\n",
        "  - Data encoding is the process of converting categorical (non-numeric) data into numeric values so that machine learning algorithms can work with it. Most machine learning models require input features to be numerical, so encoding is an essential part of data preprocessing.\n",
        "\n",
        "- Types of Data Encoding:\n",
        "\n",
        "- Label Encoding  \n",
        "Converts each category into a unique integer.  \n",
        "Example: `[\"Red\", \"Green\", \"Blue\"]` → `[2, 1, 0]`  \n",
        "Simple, but may introduce an unintended order.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "data = ['Red', 'Green', 'Blue']\n",
        "le = LabelEncoder()\n",
        "encoded = le.fit_transform(data)\n",
        "print(encoded)  # [2 1 0]\n",
        "```\n",
        "\n",
        "- One-Hot Encoding  \n",
        "Converts each category into a new binary column.  \n",
        "Avoids introducing ordinal relationships between categories.\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import numpy as np\n",
        "\n",
        "data = np.array([['Red'], ['Green'], ['Blue']])\n",
        "encoder = OneHotEncoder(sparse=False)\n",
        "encoded = encoder.fit_transform(data)\n",
        "print(encoded)\n",
        "```\n",
        "\n",
        "Output:\n",
        "```\n",
        "[[0. 0. 1.]\n",
        " [0. 1. 0.]\n",
        " [1. 0. 0.]]\n",
        "```\n",
        "\n",
        "- Ordinal Encoding  \n",
        "Similar to label encoding, but used for categories with a meaningful order.  \n",
        "Example: `[\"Low\", \"Medium\", \"High\"]` → `[0, 1, 2]`\n",
        "\n",
        "```python\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "\n",
        "data = [['Low'], ['Medium'], ['High']]\n",
        "encoder = OrdinalEncoder(categories=[['Low', 'Medium', 'High']])\n",
        "encoded = encoder.fit_transform(data)\n",
        "print(encoded)\n",
        "```\n",
        "\n",
        "\n",
        " - When to Use Which:\n",
        "- Label Encoding: For target variable or ordinal data  \n",
        "- One-Hot Encoding: For unordered categorical variables  \n",
        "- Ordinal Encoding: For ordered categorical variables\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "vp-wShesJbjY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n2i4SBUQJZUE"
      },
      "outputs": [],
      "source": []
    }
  ]
}